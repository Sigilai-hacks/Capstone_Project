{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82705845",
   "metadata": {},
   "source": [
    "Project Report: Crop Recommendation Model\n",
    "\n",
    "Executive Summary\n",
    "\n",
    "This project developed a machine learning model to recommend suitable crops for farmers based on critical agricultural parameters. The model aims to optimize farming outcomes by matching crops to specific environmental conditions, ultimately reducing losses and increasing production efficiency.\n",
    "\n",
    "Project Overview\n",
    "\n",
    "The Crop Recommendation Model was designed to solve a critical problem in agriculture: farmers often invest significant capital and resources without knowledge of which crops would thrive in their specific conditions. By analyzing parameters such as soil nutrients (N, P, K), temperature, humidity, pH levels, and rainfall, the model provides data-driven crop recommendations to maximize yields while minimizing inputs.\n",
    "\n",
    "Approach and Methodology\n",
    "\n",
    "Data Collection and Preparation\n",
    "\n",
    "Utilized a comprehensive dataset containing soil nutrients, climate conditions, and corresponding crop labels\n",
    "Initial dataset exploration revealed multiple data types (numeric and categorical) requiring different preprocessing approaches\n",
    "Conducted extensive exploratory data analysis to understand parameter distributions and relationships\n",
    "\n",
    "Here's how we loaded and initially explored the dataset:\n",
    "python# Import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading the Dataset\n",
    "\n",
    "df = pd.read_csv(\"Crop_recommendation.csv\")\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()\n",
    "print(df.dtypes)\n",
    "\n",
    "# Data Distribution Analysis\n",
    "# Analysis of Object Column\n",
    "\n",
    "print(\"Object Column Analysis:\")\n",
    "print(df['label'].value_counts())\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "Data Preprocessing\n",
    "\n",
    "Performed thorough data cleaning to handle missing values, placeholders, and anomalies\n",
    "Encoded categorical variables using One-Hot Encoding for machine readability\n",
    "Normalized numerical features with Standard Scaler to prevent dominant features\n",
    "Balanced the dataset using SMOTETomek to address class imbalance issues\n",
    "\n",
    "The data cleaning process included:\n",
    "python# Checking Missing Values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Checking NAN, N/A, Unknown Values in the dataset\n",
    "\n",
    "object_columns = df.select_dtypes(include=\"object\").columns\n",
    "placeholders = [\"NA\", \"Unknown\", \"missing\", \"N/A\"]\n",
    "placeholder_mask = df[object_columns].apply(lambda x: x.isin(placeholders))\n",
    "print(placeholder_mask.sum())\n",
    "\n",
    "# Handling Categorical Values Using One-Hot Encoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_data = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns=encoder.get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "final_df = pd.concat([df.drop(categorical_cols, axis=1), encoded_df], axis=1)\n",
    "Feature Engineering\n",
    "\n",
    "Created enhanced features to improve model performance:\n",
    "\n",
    "Nutrient ratios (N/P, N/K, P/K)\n",
    "Composite NPK scores\n",
    "pH suitability indicators\n",
    "Temperature-humidity indices\n",
    "Growing degree days (GDD) calculations\n",
    "Cumulative rainfall measurements\n",
    "Crop growth stage indicators\n",
    "\n",
    "\n",
    "\n",
    "Feature engineering implementation:\n",
    "\n",
    "python# Nutrient ratios\n",
    "df['N_P_ratio'] = df['N'] / df['P']\n",
    "df['N_K_ratio'] = df['N'] / df['K']\n",
    "df['P_K_ratio'] = df['P'] / df['K']\n",
    "\n",
    "# NPK composite score\n",
    "\n",
    "df['npk_score'] = (df['N'] * 0.4) + (df['P'] * 0.3) + (df['K'] * 0.3)\n",
    "\n",
    "# pH suitability\n",
    "\n",
    "optimal_ph = {\n",
    "    'wheat': (6.0, 7.0),\n",
    "    'rice': (5.0, 6.5),\n",
    "    'corn': (5.8, 6.8),\n",
    "}\n",
    "\n",
    "def ph_suitability(row):\n",
    "    crop = row['label']\n",
    "    min_ph, max_ph = optimal_ph.get(crop, (0, 14))\n",
    "    return 1 if min_ph <= row['ph'] <= max_ph else 0\n",
    "\n",
    "df['ph_suitability'] = df.apply(ph_suitability, axis=1)\n",
    "\n",
    "# Temperature-humidity index\n",
    "\n",
    "df['temp_humidity_index'] = df['temperature'] * df['humidity'] / 100\n",
    "\n",
    "Model Development\n",
    "\n",
    "Implemented a structured approach to model selection by testing multiple algorithms:\n",
    "\n",
    "Logistic Regression\n",
    "Decision Tree\n",
    "Random Forest\n",
    "Support Vector Machine (SVM)\n",
    "\n",
    "\n",
    "Split data strategically (70% training, 15% validation, 15% testing)\n",
    "Created robust preprocessing pipelines to handle different data types\n",
    "Conducted hyperparameter tuning using RandomizedSearchCV\n",
    "Validated models using 5-fold cross-validation\n",
    "\n",
    "Model pipeline and training:\n",
    "\n",
    "pythonfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Data Splitting\n",
    "\n",
    "X = df.drop('N', axis=1)  \n",
    "y = df['K']\n",
    "\n",
    "# First split: 70% train, 30% temp (val+test)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 15% val and 15% test\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Building preprocessing pipeline\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Full pipeline with model\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Model training\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "Model Evaluation\n",
    "\n",
    "Assessed models using multiple metrics:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "F1-score (macro average)\n",
    "ROC-AUC for multi-class classification\n",
    "\n",
    "\n",
    "Analyzed feature importance to understand key predictors\n",
    "Evaluated model fairness across different parameter groups\n",
    "\n",
    "Model evaluation and comparison:\n",
    "\n",
    "pythonfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Evaluate predictions\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC for multiclass\n",
    "\n",
    "classes = pipeline.named_steps['classifier'].classes_\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "roc_auc = roc_auc_score(y_test_bin, y_proba, multi_class='ovr', average='macro')\n",
    "print(f\"ROC-AUC (multi-class OVR): {roc_auc:.2f}\")\n",
    "\n",
    "# Comparison of different models\n",
    "\n",
    "results = []\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline with preprocessor and model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_val, y_val)\n",
    "    results[name] = score\n",
    "    print(f\"{name}: Validation Accuracy = {score:.4f}\")\n",
    "\n",
    "Challenges Encountered\n",
    "\n",
    "Data Quality Issues\n",
    "\n",
    "Missing values and potential placeholders required careful handling\n",
    "Encountered type conversion difficulties when working with categorical data\n",
    "Some preprocessing pipelines created dimensionality mismatches\n",
    "\n",
    "Technical Implementation Challenges\n",
    "\n",
    "Faced pipeline compatibility issues when combining preprocessing steps with models\n",
    "Encountered errors in SMOTE implementation due to data format inconsistencies\n",
    "ROC-AUC calculation for multi-class problems required special handling\n",
    "\n",
    "Model Performance Challenges\n",
    "\n",
    "Some models showed signs of potential overfitting\n",
    "Balancing model complexity with interpretability proved difficult\n",
    "Ensuring fairness across all parameter groups required additional analysis\n",
    "\n",
    "Key Improvements and Solutions\n",
    "\n",
    "Enhanced Data Pipeline\n",
    "\n",
    "Created a robust preprocessing pipeline handling both numerical and categorical features\n",
    "Implemented proper imputation strategies to handle missing values\n",
    "Developed a consistent approach to feature scaling and encoding\n",
    "\n",
    "Model Optimization\n",
    "\n",
    "Fine-tuned hyperparameters to improve model performance:\n",
    "\n",
    "pythonfrom sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': randint(2, 10),\n",
    "    'classifier__min_samples_leaf': randint(1, 5),\n",
    "    'classifier__max_features': ['sqrt', 'log2', None],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "    'classifier__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "Interpretability Enhancements\n",
    "\n",
    "Added LIME explainer to provide insights into individual predictions:\n",
    "\n",
    "pythonfrom lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train_imputed.values,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['class_0', 'class_1'],  # Adjust based on your classes\n",
    "    mode='classification',\n",
    "    discretize_continuous=False\n",
    ")\n",
    "\n",
    "\n",
    "instance = X_test_imputed.iloc[0].values\n",
    "explanation = explainer.explain_instance(\n",
    "    instance, \n",
    "    pipeline.predict_proba\n",
    ")\n",
    "\n",
    "Visualized feature importance:\n",
    "\n",
    "python# Feature Importance Analysis\n",
    "importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "features = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df.head(15), x='Importance', y='Feature')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Key Findings and Insights\n",
    "Model Performance\n",
    "\n",
    "Random Forest consistently outperformed other models, showing superior accuracy and F1-scores\n",
    "Tree-based models generally performed better than linear models, suggesting non-linear relationships in the data\n",
    "Cross-validation demonstrated stable performance across different data subsets\n",
    "\n",
    "Feature Importance\n",
    "\n",
    "Soil nutrients (N, P, K) showed significant influence on crop recommendations\n",
    "Environmental factors (particularly rainfall and temperature) were crucial determinants\n",
    "Composite features like nutrient ratios proved valuable for prediction\n",
    "\n",
    "Agricultural Insights\n",
    "\n",
    "Different crops showed distinct requirements for optimal growth conditions\n",
    "Certain parameter combinations were strongly predictive of specific crop suitability\n",
    "The relationship between soil pH and crop suitability was confirmed as a critical factor\n",
    "\n",
    "Recommendations for Implementation\n",
    "\n",
    "Deploy as Decision Support Tool: Implement the model as a practical tool for farmers to consult before planting decisions\n",
    "Continuous Improvement: Set up mechanisms to gather feedback from actual crop outcomes to refine the model\n",
    "Localization: Consider regional adaptations to account for local agricultural practices and conditions\n",
    "User Interface Development: Create an accessible interface that translates complex model outputs into actionable recommendations\n",
    "Integration with Other Systems: Connect with weather forecasting and soil testing services for real-time recommendations\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The Crop Recommendation Model successfully addresses a critical agricultural challenge by providing data-driven guidance for crop selection. By analyzing soil and environmental parameters, the model helps farmers optimize resource allocation and increase productivity. The Random Forest algorithm proved most effective for this application, demonstrating strong predictive performance while maintaining interpretability. With proper implementation, this model has significant potential to improve agricultural outcomes and promote sustainable farming practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
